{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HKPPy6X4Nac"
      },
      "source": [
        "# Full Stack Practice of LLM Training - LLM Data Curation @ RLChina 2024\n",
        "\n",
        "- Author: [Cheng Deng](https://www.cdeng.net/)[‚úâÔ∏è]((davendw49@gmail.com), [Jun Wang](http://www0.cs.ucl.ac.uk/staff/jun.wang/)\n",
        "\n",
        "---\n",
        "## Main Task\n",
        "\n",
        "In this section, we will set up data preprocessing, including improving data quality and deduplication, in preparation for large language model training. Following this, we will develop the input pipeline by implementing a text tokenizer and creating a custom PyTorch DataLoader specifically designed for our LLM.\n",
        "\n",
        "![](https://www.cdeng.net/resources/imgs/RLChina24/a.png)\n",
        "\n",
        "Here is the prerequisite knowledge required:\n",
        "\n",
        "- Python File Processing\n",
        "- Regex\n",
        "\n",
        "First, let's do some of the preparation, including the essential package installation. üòÅ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvGhOv1zyZp_"
      },
      "outputs": [],
      "source": [
        "!pip install sciparser\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y79lPCgUyaDb"
      },
      "source": [
        "## 1. Choose Data\n",
        "\n",
        "First, like many other blogs, we introduce the idea to gather data. Choosing the right data for LLM (Large Language Model) pretraining is crucial for model performance, relevance, and generalization. Pretraining data directly impacts the knowledge, linguistic understanding, and contextual awareness of the model, so selecting diverse, high-quality, and appropriately sized datasets is essential.\n",
        "\n",
        "Here's a guide to help you make informed decisions when choosing data for LLM pretraining:\n",
        "\n",
        "### Key Criteria for Choosing Data\n",
        "\n",
        "1. **Domain Relevance**\n",
        "   - **General Purpose Models** (like GPT-3, BERT): Use diverse, broad-domain datasets covering a wide variety of topics, styles, and genres (e.g., books, websites, news articles).\n",
        "   - **Domain-Specific Models** (like medical, legal, or geoscience models): Use datasets specific to the target domain, ensuring that the language and concepts relevant to the domain are well-represented.\n",
        "\n",
        "2. **Size and Diversity**\n",
        "   - Pretraining LLMs generally requires **massive datasets** (terabytes of data). The data should be diverse in terms of:\n",
        "     - **Topics**: A wide range of subjects (e.g., science, technology, arts, history).\n",
        "     - **Genres**: Books, blogs, forums, research papers, news articles, etc.\n",
        "     - **Languages**: If creating a multilingual model, ensure a balanced mix of languages.\n",
        "   - **High-Quality Sources**: Collect data from sources known for high-quality language use (e.g., books, academic papers, Wikipedia).\n",
        "\n",
        "3. **Data Quality**\n",
        "   - **Clean and Well-Formatted**: Data should be well-structured, with minimal noise (e.g., broken sentences, unrecognized characters, incomplete text).\n",
        "   - **Balanced**: Avoid over-representation of certain topics, genres, or styles that might bias the model.\n",
        "   - **Appropriate Length**: Use documents that allow the model to learn from long contexts, such as entire books or articles.\n",
        "\n",
        "4. **Ethical Considerations**\n",
        "   - **Bias and Fairness**: Make sure that the dataset isn‚Äôt skewed towards certain demographics, cultures, or ideologies. Models trained on biased data can reflect or even amplify these biases.\n",
        "   - **Sensitive Data**: Avoid including personally identifiable information (PII) or confidential/sensitive data. This is crucial for privacy and security.\n",
        "   - **Diversity**: Include datasets representing different social groups, cultures, and backgrounds to ensure fairness and inclusivity in the model.\n",
        "\n",
        "5. **Scalability and Accessibility**\n",
        "   - Ensure that the data can scale to the size required for pretraining (typically in the range of hundreds of billions of tokens).\n",
        "   - Use publicly available datasets or data sources that have open licenses, so that the data can be used and redistributed without legal restrictions.\n",
        "\n",
        "And this notebook, we go through the way to train a model to classify the quality of data involves several key steps, from defining what \"quality\" means for your specific use case to gathering labeled training data and choosing the right machine learning model.\n",
        "\n",
        "\n",
        "First, we **Define Data Quality Criteria**\n",
        "\n",
        "Before starting the process, it's essential to define what \"quality\" means in the context of your dataset. Common criteria for data quality include:\n",
        "\n",
        "- **Completeness**: Whether the data is missing any critical components.\n",
        "- **Consistency**: Whether the data follows the same format or structure across all entries.\n",
        "- **Accuracy**: Whether the data accurately represents the real-world entities it refers to.\n",
        "- **Validity**: Whether the data conforms to defined rules (e.g., date formats, numerical ranges).\n",
        "- **Uniqueness**: Whether there are duplicate entries.\n",
        "- **Bias**: Whether the data exhibits systematic bias that could affect downstream applications.\n",
        "  \n",
        "You may need to focus on one or multiple aspects of data quality depending on the requirements of your use case.\n",
        "\n",
        "Second we **Collect and Label Training Data**\n",
        "\n",
        "To build a supervised model, you‚Äôll need a labeled dataset with examples of \"high quality\" and \"low quality\" data. You can collect this in two ways:\n",
        "\n",
        "- **Manual Labeling**: Human annotators evaluate samples of data and label them as \"high quality\" or \"low quality\" based on predefined criteria.\n",
        "  \n",
        "- **Rule-Based Labeling**: Automatically label data using heuristic rules. For example, missing values, inconsistencies, or outliers can serve as indicators of low quality. Use these rules to label data automatically.\n",
        "\n",
        "#### Example Data Quality Labeling:\n",
        "\n",
        "For each sample (e.g., text, numerical data, image), you might assign a quality label:\n",
        "\n",
        "| Data Sample | Quality Label |\n",
        "|-------------|---------------|\n",
        "| \"John, Doe, 12/12/1995\" | High Quality |\n",
        "| \"Jhn, , 1995/12/12\" | Low Quality |\n",
        "| \"J.Do, ?19\" | Low Quality |\n",
        "| \"Jane, Doe, 01/01/2000\" | High Quality |\n",
        "\n",
        "Here we refer to *DCLM-baseline-1.0* to train a model to classify the data based on their quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omshhs25NGGM"
      },
      "source": [
        "To implement the approach (**fastText OH-2.5 + ELI5** ) in DCLM's paper, we will need to train a fastText model using labeled instruction-formatted data drawn from **OpenHermes 2.5 (OH-2.5)** and high-scoring posts from the **r/ExplainLikeImFive (ELI5)** subreddit. Below is the full code to prepare, train, and evaluate a fastText model on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PUdaAAP1sco"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Step 1: Prepare the OH-2.5 + ELI5 Dataset\n",
        "\n",
        "def prepare_fasttext_format(dataset, file_name):\n",
        "    \"\"\"\n",
        "    Convert the dataset into fastText format where each line starts with a label.\n",
        "    :param dataset: Hugging Face dataset with 'text' and 'label' columns.\n",
        "    :param file_name: File name to save the formatted data.\n",
        "    \"\"\"\n",
        "    with open(file_name, 'w') as f:\n",
        "        for example in dataset:\n",
        "            label = \"__label__\" + example['label']  # Labels in fastText format (e.g., '__label__high_quality')\n",
        "            text = example['text'].replace('\\n', ' ')  # Replace newlines with spaces\n",
        "            f.write(f\"{label} {text}\\n\")\n",
        "\n",
        "# Load OpenHermes 2.5 and ELI5 datasets (replace with actual paths or dataset loading methods)\n",
        "# For the example, let's assume you have datasets OH2.5 and ELI5 already loaded.\n",
        "def load_datasets():\n",
        "    oh25 = load_dataset(\"openhermes_2_5\")  # Replace with actual OH-2.5 loading method\n",
        "    eli5 = load_dataset(\"eli5\")  # Replace with actual ELI5 loading method\n",
        "    return oh25['train'], eli5['train']\n",
        "\n",
        "# Combine OH-2.5 and ELI5 datasets\n",
        "def combine_datasets(oh25_dataset, eli5_dataset):\n",
        "    combined_data = []\n",
        "\n",
        "    # Add OH-2.5 data (assuming 'label' and 'text' fields)\n",
        "    for example in oh25_dataset:\n",
        "        combined_data.append({\n",
        "            'label': example['label'],\n",
        "            'text': example['text']\n",
        "        })\n",
        "\n",
        "    # Add ELI5 data (assuming 'label' and 'text' fields)\n",
        "    for example in eli5_dataset:\n",
        "        combined_data.append({\n",
        "            'label': example['label'],\n",
        "            'text': example['text']\n",
        "        })\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "# Step 2: Format the Combined Data for fastText\n",
        "\n",
        "def save_dataset_for_fasttext(oh25, eli5, output_file):\n",
        "    combined_data = combine_datasets(oh25, eli5)\n",
        "    prepare_fasttext_format(combined_data, output_file)\n",
        "\n",
        "# Step 3: Train the fastText Classifier\n",
        "\n",
        "def train_fasttext_classifier(training_file, model_save_path):\n",
        "    \"\"\"\n",
        "    Train the fastText classifier using instruction-formatted OH-2.5 + ELI5 data.\n",
        "    :param training_file: Path to the training file in fastText format.\n",
        "    :param model_save_path: Path to save the trained fastText model.\n",
        "    \"\"\"\n",
        "    # Train the fastText model\n",
        "    model = fasttext.train_supervised(\n",
        "        input=training_file,\n",
        "        epoch=10,  # Number of training epochs\n",
        "        lr=1.0,  # Learning rate\n",
        "        wordNgrams=2,  # Use bigrams\n",
        "        dim=100,  # Dimension of word vectors\n",
        "        loss='softmax'  # Loss function\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_model(model_save_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 4: Evaluate the Model\n",
        "\n",
        "def evaluate_model(model, test_file):\n",
        "    \"\"\"\n",
        "    Evaluate the fastText model on a test dataset.\n",
        "    :param model: Trained fastText model.\n",
        "    :param test_file: Test file in fastText format.\n",
        "    \"\"\"\n",
        "    # Evaluate the model using fastText's built-in evaluation function\n",
        "    result = model.test(test_file)\n",
        "\n",
        "    # fastText returns:\n",
        "    # (number of examples, precision@1, recall@1)\n",
        "    print(f\"Number of examples: {result[0]}\")\n",
        "    print(f\"Precision@1: {result[1]}\")\n",
        "    print(f\"Recall@1: {result[2]}\")\n",
        "\n",
        "# Step 5: Main Function to Run Everything\n",
        "\n",
        "def main():\n",
        "    # Load datasets\n",
        "    oh25_data, eli5_data = load_datasets()\n",
        "\n",
        "    # Prepare data for fastText\n",
        "    training_file = 'oh25_eli5_train.txt'\n",
        "    save_dataset_for_fasttext(oh25_data, eli5_data, training_file)\n",
        "\n",
        "    # Train the fastText classifier\n",
        "    model_save_path = 'fasttext_oh25_eli5.bin'\n",
        "    model = train_fasttext_classifier(training_file, model_save_path)\n",
        "\n",
        "    # Optionally, prepare a test set and evaluate the model\n",
        "    test_file = 'oh25_eli5_test.txt'  # You should create a similar test set\n",
        "    evaluate_model(model, test_file)\n",
        "\n",
        "# Run the process\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvlvfmRhNsIz"
      },
      "source": [
        "As we can see above, we do 4 step training.\n",
        "\n",
        "1. **Dataset Loading**:\n",
        "   - The `load_datasets` function loads both OpenHermes 2.5 (OH-2.5) and r/ExplainLikeImFive (ELI5) datasets.\n",
        "   - Replace the `load_dataset(\"openhermes_2_5\")` and `load_dataset(\"eli5\")` with the actual paths or dataset loading methods you are using.\n",
        "\n",
        "2. **Data Preparation**:\n",
        "   - We combine OH-2.5 and ELI5 into a single dataset using `combine_datasets`.\n",
        "   - The `prepare_fasttext_format` function formats the combined dataset into the fastText format, with labels (`__label__high_quality`, `__label__low_quality`) at the beginning of each line.\n",
        "\n",
        "3. **Training the fastText Model**:\n",
        "   - The `train_fasttext_classifier` function trains a fastText classifier using the formatted dataset.\n",
        "   - It trains using **supervised learning** with hyperparameters like `epoch=10`, `wordNgrams=2`, and `lr=1.0`. You can adjust these values to experiment with model performance.\n",
        "\n",
        "4. **Model Evaluation**:\n",
        "   - After training, the `evaluate_model` function evaluates the trained model on a test set, printing out the precision and recall metrics.\n",
        "\n",
        "And the following thing is waiting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylk9b68l8YuA"
      },
      "source": [
        "## 2. Data Cleaning\n",
        "\n",
        "\"In most cases, the data sources may come from websites, scanned books, documents stored in the form of PDFs, and other similar data formats. For example, the following is a website-style text, which is readable but unclean. We have to remove the tags in HTML like `<a>` and `\\`, as well as non-ASCII characters like `ÔΩú`.\"\n",
        "\n",
        "*A possible meaning for \"Cosmopedia\" could be an encyclopedi\\<a\\> or collection of inform@tion about different cultures, soci\\eties, and topics from around the wor|ld, emphasizing diversity and global connec tedness.*\n",
        "\n",
        "So we have to do data cleaning to remove the tokens we do not expect and filter the context too short.\n",
        "\n",
        "### 2.1 **Data Filtering Based on Quality**\n",
        "It's important to remove low-quality, irrelevant, or harmful content. This could involve filtering out texts that:\n",
        "- Contain excessive typos or grammatical errors.\n",
        "- Are too short or too long.\n",
        "- Contain offensive, biased, or harmful language (important for ethical AI development).\n",
        "\n",
        "**Techniques:**\n",
        "- **Language Detection**: Ensure that only data in the target language is included.\n",
        "- **Readability Scoring**: Use Flesch-Kincaid or other readability scores to filter out overly complex or nonsensical sentences.\n",
        "- **Bias Detection**: Use automated tools to detect and remove biased or harmful content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh5cjlvO8b65"
      },
      "outputs": [],
      "source": [
        "def filter_low_quality(texts, min_length=20, max_length=300):\n",
        "  return [text for text in texts if min_length < len(text.split()) < max_length]\n",
        "\n",
        "filtered_texts = filter_low_quality(list_of_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKFdwSQnN3KT"
      },
      "source": [
        "### **2.2 Removing HTML Tags**\n",
        "\n",
        "- Option A: Using Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7qNNQeKQLaY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove HTML tags using regular expressions.\"\"\"\n",
        "    clean = re.compile('<.*?>')\n",
        "    return re.sub(clean, '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FxorgrcQOga"
      },
      "source": [
        "- Option B: Using BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH5BOIpZQVqN"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove HTML tags using BeautifulSoup.\"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2zBefeXSOnT"
      },
      "source": [
        " - Option C: Remove HTML Tags or Special Characters\n",
        "   - If the text contains HTML tags or artifacts (common when extracting from websites), remove them using a library like `BeautifulSoup` or regex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "698b8Waja8aZ"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "cleaned_text = remove_html_tags(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5a_IRK9QYI7"
      },
      "source": [
        "### **2.3 Removing Backslashes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZiKL_uRQeQS"
      },
      "outputs": [],
      "source": [
        "def remove_backslashes(text):\n",
        "    \"\"\"Remove backslashes from the text.\"\"\"\n",
        "    return text.replace('\\\\', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQPVllkQhBW"
      },
      "source": [
        "### **2.4 Removing Non-ASCII Characters**\n",
        "\n",
        "- Option A: Using Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dTO1_pMQnKT"
      },
      "outputs": [],
      "source": [
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters by encoding and decoding.\"\"\"\n",
        "    return text.encode('ascii', 'ignore').decode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvNk3iMNQukA"
      },
      "source": [
        "- Option B: Using Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJm0PFyZQxIL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters using regular expressions.\"\"\"\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF1YlQpYSqdj"
      },
      "source": [
        "- Option C: Using unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58sLgKnSSoli"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    return unidecode.unidecode(text)\n",
        "\n",
        "cleaned_text = remove_non_ascii(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0eK3jK9Rr3X"
      },
      "source": [
        "### 2.5 **Fix Encoding Issues**\n",
        "   - Text extracted from PDFs often has incorrect encodings or symbols (e.g., replacing `a` with `…ë` or `d` with `‘Å`). Use libraries like `ftfy` to fix encoding issues automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRHEWqB4Xp7q"
      },
      "outputs": [],
      "source": [
        "from ftfy import fix_text\n",
        "\n",
        "def fix_encoding_issues(text):\n",
        "   return fix_text(text)\n",
        "\n",
        "cleaned_text = fix_encoding_issues(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z593ZrWRuRi"
      },
      "source": [
        "### 2.6 **Remove or Replace Special Characters**\n",
        "   - Strip out unwanted special characters or replace them with their proper forms. You can use regex to target specific characters that shouldn't be in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m2Kj2woXtoI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_special_chars(text):\n",
        "   return re.sub(r'[^A-Za-z0-9\\s.,!?\\'\"]', '', text)  # Keeps common punctuation marks\n",
        "\n",
        "cleaned_text = remove_special_chars(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77jlKWCZRw3p"
      },
      "source": [
        "### 2.7 **Normalize Whitespace**\n",
        "   - Extra spaces, newlines, or tabs may appear in unclean text. Normalize the whitespace to a single space or strip extra spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eezAEFYvXz0P"
      },
      "outputs": [],
      "source": [
        "def normalize_whitespace(text):\n",
        "   return \" \".join(text.split())\n",
        "\n",
        "cleaned_text = normalize_whitespace(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiVs-dE3R0q3"
      },
      "source": [
        "### 2.8 **Fix Spelling and Grammar**\n",
        "   - Use a spell-checker to fix common spelling mistakes, which are prevalent in noisy data. Libraries like `pyspellchecker` can help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjYciCvBX3bv"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "def correct_spelling(text):\n",
        "   spell = SpellChecker()\n",
        "   corrected_text = []\n",
        "   for word in text.split():\n",
        "       corrected_text.append(spell.correction(word))\n",
        "   return \" \".join(corrected_text)\n",
        "\n",
        "cleaned_text = correct_spelling(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lr-X5HPR3ZF"
      },
      "source": [
        "\n",
        "### 2.9 **Convert to Lowercase**\n",
        "   - Converting the text to lowercase ensures uniformity when performing NLP tasks like tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLHrATVjX5wt"
      },
      "outputs": [],
      "source": [
        "def to_lowercase(text):\n",
        "   return text.lower()\n",
        "\n",
        "cleaned_text = to_lowercase(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpSNDEyiR7OJ"
      },
      "source": [
        "### 2.10 **Remove Stopwords (Optional)**\n",
        "   - Depending on the task (e.g., text classification), you might want to remove common stopwords to reduce noise. Libraries like `nltk` or `spaCy` provide predefined stopword lists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM2cZwScX8TY"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "   words = text.split()\n",
        "   return ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "cleaned_text = remove_stopwords(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb5L7LUBR_I8"
      },
      "source": [
        "### 2.11. **Remove URLs, Emails, or Phone Numbers**\n",
        "   - If the text contains unwanted URLs, emails, or phone numbers, regex can help you target and remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnDMfXnhX-rS"
      },
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "   return re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "def remove_emails(text):\n",
        "   return re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "cleaned_text = remove_urls(cleaned_text)\n",
        "cleaned_text = remove_emails(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FiwohyhSAyY"
      },
      "source": [
        "### 2.12 **Tokenization and Lemmatization (Optional)**\n",
        "   - If you are preparing data for NLP tasks, tokenizing and lemmatizing words can be useful to break the text into meaningful units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOtGwRLAYBC5"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "   doc = nlp(text)\n",
        "   return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "cleaned_text = lemmatize_text(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uKH5p1OSEbe"
      },
      "source": [
        "### Putting it all together\n",
        "You can create a preprocessing pipeline by combining these steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjjh5nqaQ5dr"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = fix_encoding_issues(text)\n",
        "    text = remove_special_chars(text)\n",
        "    text = normalize_whitespace(text)\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_urls(text)\n",
        "    text = remove_emails(text)\n",
        "    text = to_lowercase(text)\n",
        "    return text\n",
        "\n",
        "raw_text = 'Your raw, unclean text extracted from PDF or website here.'\n",
        "cleaned_text = clean_text(raw_text)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf5BQP61Q6mF"
      },
      "source": [
        "### Tools and Libraries to Use:\n",
        "- **`ftfy`** for fixing encoding errors.\n",
        "- **`BeautifulSoup`** for removing HTML tags.\n",
        "- **`re`** (regex) for custom text replacements.\n",
        "- **`nltk`/`spaCy`** for tokenization, stopword removal, and lemmatization.\n",
        "- **`unidecode`** for ASCII normalization.\n",
        "\n",
        "## 3. Deal with PDF\n",
        "\n",
        "### 3.1 Common methods\n",
        "\n",
        "Then, we choose a specific data format -- PDF. For PDFs, you can use libraries like `PyPDF2` or `pdfminer.six`. Here, we share the code of `PyPDF2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0Tj8mW2RLST"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    text = ''\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqM0lbQVY2GG"
      },
      "source": [
        "For scanned documents (images), use OCR tools like `pytesseract`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDkXf5JMZBGy"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    \"\"\"Extract text from an image using OCR.\"\"\"\n",
        "    text = pytesseract.image_to_string(Image.open(image_path))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAigoECUZAzh"
      },
      "source": [
        "And the full workflow example as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfmvyVGcZUvC"
      },
      "outputs": [],
      "source": [
        "def process_document(file_path):\n",
        "    \"\"\"Extract and clean text from a document.\"\"\"\n",
        "    # Determine file type and extract text accordingly\n",
        "    if file_path.endswith('.pdf'):\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "    elif file_path.endswith(('.png', '.jpg', '.jpeg')):\n",
        "        text = extract_text_from_image(file_path)\n",
        "    else:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "    # Clean the extracted text\n",
        "    cleaned_text = clean_text(text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Example usage\n",
        "file_path = 'sample_document.pdf'\n",
        "cleaned_text = process_document(file_path)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rd52p3sOOzl"
      },
      "source": [
        "### 3.2 Using Sciparser for scientific document\n",
        "\n",
        "Here introduce a repo call [sciparser](https://github.com/davendw49/sciparser). This repo contains a PDF parsing toolkit for preparing text corpus to transfer PDF to Markdown. Based on [PDF Parser ToolKits](https://github.com/Acemap/pdf_parser), gathering most-use PDF OCR tools for academic papers, and inspired by `grobid_tei_xml`, an open-sourced PyPI package, we develop sciparser 1.0 for text corpus pre-processing, in recent works like [K2](https://github.com/davendw49/k2) and [GeoGalactica](https://github.com/davendw49/geogalactica), we use this tool and upgrade grobid backend solution to pre-process the text corpus. Moreover, the online demo is publicly available.\n",
        "\n",
        "- Try [DEMO](https://sciparser.acemap.info/)\n",
        "\n",
        "In this repo and demo, we only share the secondary processing solution on Grobid. In the near future, we will share the multiple-backend combination solution on PDF parsing.\n",
        "\n",
        "# Requirements\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/Acemap/pdf_parser.git\n",
        "cd pdf_parser\n",
        "pip install -r requirements.txt\n",
        "python setup install\n",
        "\n",
        "git clone https://github.com/davendw49/sciparser.git\n",
        "cd sciparser\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "collapsed": true,
        "id": "k2W3kN4mOODS",
        "outputId": "85c15968-dfa0-4e9e-b36e-7fe77abaf6ef"
      },
      "outputs": [],
      "source": [
        "from pipeline import pipeline\n",
        "data = pipeline('/path/to/your/pdf/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omJjDsdhO120"
      },
      "source": [
        "For the [pdf_parser](https://github.com/Acemap/pdf_parser), here is the tips:\n",
        "Backend‚Üì / Type‚Üí | text | image | reference\n",
        ":-: | :-: | :-: | :-:\n",
        "grobid | <font color=\"#00FF00\">‚àö</font> | <font color=\"#FF0000\">√ó</font> | <font color=\"#FF0000\">√ó</font>\n",
        "cermine | <font color=\"#00FF00\">‚àö</font> | <font color=\"#00FF00\">‚àö</font> | <font color=\"#FF0000\">√ó</font>\n",
        "scienceparse | <font color=\"#00FF00\">‚àö</font> | <font color=\"#FF0000\">√ó</font> | <font color=\"#FF0000\">√ó</font>\n",
        "pdffigures | <font color=\"#FF0000\">√ó</font> | <font color=\"#00FF00\">‚àö</font> | <font color=\"#FF0000\">√ó</font>\n",
        "pdffigures2 | <font color=\"#00FF00\">‚àö</font> | <font color=\"#00FF00\">‚àö</font> | <font color=\"#FF0000\">√ó</font>\n",
        "\n",
        "## Detail demand\n",
        "\n",
        "Backend‚Üì / Requirements‚Üí | OS | java | Other\n",
        ":-: | :-: | :-: | :-:\n",
        "grobid | All (Windows/Linux/Mac) | Not Need | No\n",
        "cermine | All (Windows/Linux/Mac) | Need | No\n",
        "scienceparse | All (Windows/Linux/Mac) | Need | No\n",
        "pdffigures | Linux/Mac | Not Need | leptonica & poppler (Ubuntu: sudo apt install libpoppler-private-dev libleptonica-dev)\n",
        "pdffigures2 | All (Windows/Linux/Mac) | Need | No\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "QROWz-UMPHGb",
        "outputId": "ca852e89-48ac-4913-93de-8fb80d4a0479"
      },
      "outputs": [],
      "source": [
        "# The Parser class takes the `backend` parameter to specify the backend to use.\n",
        "# pdf_parser.Parser(backend='grobid')\n",
        "# To parse the structural information of all PDF files in the `input_dir` and save the results to `output_dir`, use the following command:\n",
        "pdf_parser.Parser.parse('text', input_dir, output_dir, n_threads=0)\n",
        "\n",
        "# To parse the image information of all PDF files in the `input_dir` and save the results to `output_dir`, use the following command:\n",
        "pdf_parser.Parser.parse('figure', input_dir, output_dir, n_threads=0)\n",
        "# Note: The `n_threads` parameter specifies the number of threads to use for parsing. The default value is **0**, which means it will use all available `CPU cores`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3U4U4PU8c94"
      },
      "source": [
        "## 4. Data Deduplication\n",
        "\n",
        "Usually, we have multiple data sources, the duplication between datasets are intuitively exsit, so we need to deduplicate the datasets to make sure the fairness between the data samples.\n",
        "**Example:**\n",
        "```python\n",
        "def deduplicate_data(texts):\n",
        "    return list(set(texts))\n",
        "\n",
        "deduplicated_texts = deduplicate_data(list_of_texts)\n",
        "```\n",
        "\n",
        "There are more advanced methods for deduplication that identify semantically similar content beyond exact matching, using techniques like **MinHash** or **TF-IDF**.\n",
        "\n",
        "In most of the time, we can use **SimHash** algorithm to finish our tasks. Here are the main steps:\n",
        "\n",
        "- **Step 1**: Load two datasets using Hugging Face's datasets library.\n",
        "- **Step 2**: Compute SimHash for each text in both datasets.\n",
        "- **Step 3**: Deduplicate the datasets by comparing SimHashes and removing duplicates based on a Hamming distance threshold.\n",
        "- **Step 4**: Combine the unique examples from both datasets into a new dataset.\n",
        "- **Step 5**: Optionally, save the combined dataset to disk for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5p5oszJ8eLw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from tqdm import tqdm\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNC88O6uQx0q"
      },
      "source": [
        "### 4.1 SimHash\n",
        "\n",
        "First we learn the SimHash here. SimHash is a hashing technique that captures the essence of the text, making it easier to compare the similarity between texts.\n",
        "\n",
        "**Step-by-Step Implementation of SimHash:**\n",
        "\n",
        "- **Tokenize the Text:** Convert the text into words or tokens.\n",
        "- **Hash Each Token:** Use a hash function (e.g., hash()) to convert each token into an integer.\n",
        "- **Vector Representation:** Convert each hash value into a binary vector. If the bit at a position is 1, increase a corresponding position in a counter. If the bit is 0, decrease the value.\n",
        "- **Generate SimHash:** After processing all tokens, create a final fingerprint by taking the sign of each bit position in the vector (positive or negative sum of bit positions).\n",
        "- **Hamming Distance:** Compare two SimHash values using the Hamming distance (number of differing bits between two binary representations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCS_heNaQxT3"
      },
      "outputs": [],
      "source": [
        "class Simhash:\n",
        "    def __init__(self, f=64):\n",
        "        self.f = f\n",
        "\n",
        "    def _hash(self, token):\n",
        "        return int(hashlib.md5(token.encode('utf-8')).hexdigest(), 16)\n",
        "\n",
        "    def compute_simhash(self, text):\n",
        "        tokens = text.split()\n",
        "        v = [0] * self.f\n",
        "        for token in tokens:\n",
        "            h = self._hash(token)\n",
        "            for i in range(self.f):\n",
        "                bitmask = 1 << i\n",
        "                if h & bitmask:\n",
        "                    v[i] += 1\n",
        "                else:\n",
        "                    v[i] -= 1\n",
        "        fingerprint = 0\n",
        "        for i in range(self.f):\n",
        "            if v[i] > 0:\n",
        "                fingerprint |= 1 << i\n",
        "        return fingerprint\n",
        "\n",
        "    def hamming_distance(self, simhash1, simhash2):\n",
        "        x = simhash1 ^ simhash2\n",
        "        tot = 0\n",
        "        while x:\n",
        "            tot += x & 1\n",
        "            x >>= 1\n",
        "        return tot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quc_-tJmVTY2"
      },
      "source": [
        "Now, let's analysis the time and space complexity of the algorithm.\n",
        "\n",
        "**Time and Space Complexity Analysis of the SimHash**\n",
        "\n",
        "1. **Time Complexity**:\n",
        "\n",
        "The text is split into tokens, which takes $O(n)$ time, where $n$ is the number of characters in the input string (assuming space-based splitting). For each token, a hash is computed. Hashing a single token takes $O(1)$ because hashing has constant time complexity. For each of the $m$ tokens, we process each bit of the hash value, and the hash value has $f$ bits (e.g., 64), size of the fingerprint. Therefore, processing all tokens takes $O(m \\times f)$ time, .\n",
        "  \n",
        "**Total Time Complexity**: The overall time complexity is $O(n) + O(m \\times f)$, but since $m$ is proportional to $n$, this simplifies to **$O(n \\times f)$**.\n",
        "\n",
        "2. **Space Complexity**:\n",
        "\n",
        "The space used for the vector representation of the hash is $O(f)$, where $f$ is the number of bits in the fingerprint (e.g., 64 bits or 128 bits). Then Storing the hash values takes $O(m)$, where $m$ is the number of tokens in the text.\n",
        "  \n",
        "**Total Space Complexity**: The overall space complexity is **$O(m + f)$**, which typically simplifies to **$O(m)$**, since $f$ (the number of bits) is constant (e.g., 64 bits).\n",
        "\n",
        "3. Comparison Deployment Complexity:\n",
        "\n",
        "**Hamming Distance**: Comparing two SimHash fingerprints using Hamming distance involves checking each of the \\(f\\) bits in the two fingerprints. Therefore, the time complexity of Hamming distance computation is **$O(f)$**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtxjrM8-V2ZY"
      },
      "outputs": [],
      "source": [
        "# Load Hugging Face datasets\n",
        "def load_datasets(dataset1_name, dataset2_name):\n",
        "    dataset1 = load_dataset(dataset1_name)\n",
        "    dataset2 = load_dataset(dataset2_name)\n",
        "    return dataset1, dataset2\n",
        "\n",
        "# Deduplicate datasets based on SimHash\n",
        "def deduplicate_datasets(dataset1, dataset2, threshold=5):\n",
        "    simhash = Simhash()\n",
        "\n",
        "    simhashes_dataset1 = {idx: simhash.compute_simhash(item['text']) for idx, item in enumerate(dataset1)}\n",
        "    simhashes_dataset2 = {idx: simhash.compute_simhash(item['text']) for idx, item in enumerate(dataset2)}\n",
        "\n",
        "    unique_dataset1 = []\n",
        "    unique_dataset2 = []\n",
        "\n",
        "    # Process dataset1 and remove duplicates\n",
        "    for idx1, simhash1 in tqdm(simhashes_dataset1.items(), desc=\"Processing dataset1\"):\n",
        "        is_duplicate = False\n",
        "        for idx2, simhash2 in simhashes_dataset2.items():\n",
        "            if simhash.hamming_distance(simhash1, simhash2) < threshold:\n",
        "                is_duplicate = True\n",
        "                break\n",
        "        if not is_duplicate:\n",
        "            unique_dataset1.append(dataset1[idx1])\n",
        "\n",
        "    # Process dataset2 and remove duplicates\n",
        "    for idx2, simhash2 in tqdm(simhashes_dataset2.items(), desc=\"Processing dataset2\"):\n",
        "        is_duplicate = False\n",
        "        for idx1, simhash1 in simhashes_dataset1.items():\n",
        "            if simhash.hamming_distance(simhash2, simhash1) < threshold:\n",
        "                is_duplicate = True\n",
        "                break\n",
        "        if not is_duplicate:\n",
        "            unique_dataset2.append(dataset2[idx2])\n",
        "\n",
        "    return unique_dataset1, unique_dataset2\n",
        "\n",
        "# Combine two datasets into one\n",
        "def combine_datasets(dataset1, dataset2):\n",
        "    combined_dataset = dataset1 + dataset2\n",
        "    # If using Hugging Face `datasets`:\n",
        "    return Dataset.from_dict(combined_dataset)\n",
        "\n",
        "# Main function to load, deduplicate, and combine datasets\n",
        "def do_deduplication(dataset1_name, dataset2_name):\n",
        "    # Load the datasets\n",
        "    dataset1, dataset2 = load_datasets(dataset1_name, dataset2_name)\n",
        "\n",
        "    # Deduplicate the datasets\n",
        "    unique_dataset1, unique_dataset2 = deduplicate_datasets(dataset1['train'], dataset2['train'])\n",
        "\n",
        "    print(f\"Unique records in dataset1: {len(unique_dataset1)}\")\n",
        "    print(f\"Unique records in dataset2: {len(unique_dataset2)}\")\n",
        "\n",
        "    # Combine the two datasets\n",
        "    combined_dataset = combine_datasets(unique_dataset1, unique_dataset2)\n",
        "\n",
        "    # Save or return the combined dataset as necessary\n",
        "    print(f\"Combined dataset length: {len(combined_dataset)}\")\n",
        "    combined_dataset.save_to_disk('combined_dataset')  # Save to disk if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfqPtpbEV77-"
      },
      "source": [
        "Then we can simply test the data deduplication using home-make SimHash algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99gtYSZoWFP0"
      },
      "outputs": [],
      "source": [
        "dataset1_name = \"cosmopedia-test-1\"\n",
        "dataset2_name = \"cosmopedia-test-2\"\n",
        "do_deduplication(dataset1_name, dataset2_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLEFyVj_8eoo"
      },
      "source": [
        "Summary of Methods:\n",
        "- **Data augmentation** and **deduplication** ensure diversity and uniqueness in the data.\n",
        "- **Noise injection** and **dynamic masking** improve robustness.\n",
        "- **Data filtering** and **balancing** ensure quality and fairness.\n",
        "- **Custom tokenization and semantic clustering** optimize token handling and coverage.\n",
        "- **Adversarial training** strengthens the model against challenging input.\n",
        "\n",
        "## 5. More deeper in Data preprocessing\n",
        "\n",
        "### 5.1 **Data Augmentation**\n",
        "   - Augment your dataset by introducing variations or transformations to increase the diversity and size of the training data.\n",
        "   - **Paraphrasing**: Use paraphrasing techniques to generate multiple variants of the same text while preserving meaning.\n",
        "   - **Synonym Substitution**: Replace certain words with their synonyms.\n",
        "   - **Back-translation**: Translate text to another language and back to the original to generate different expressions of the same content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxsrr0--Yqfo"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def back_translate(text, src_lang='en', tgt_lang='fr'):\n",
        "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
        "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "back_translated_text = back_translate(original_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czpg3P_rZrMx"
      },
      "source": [
        "### 5.2 **Noise Injection (Robustness Training)**\n",
        "   - Purposefully injecting noise (e.g., spelling mistakes, missing punctuation, or small semantic errors) into the training data to make the model more robust to noisy input. This helps LLMs generalize better to real-world, uncleaned data.\n",
        "\n",
        "   **Methods for Noise Injection:**\n",
        "   - **Spelling perturbations**: Introduce typos or missing characters.\n",
        "   - **Grammatical errors**: Introduce small grammatical inconsistencies.\n",
        "   - **Random word swaps**: Swap adjacent words randomly.\n",
        "\n",
        "   But actually, we usually do this in some SFT way, teaching the model to learn to clarify output with supervised data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWzmvEySZrg2"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_word_swap(text):\n",
        "    words = text.split()\n",
        "    idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "    return \" \".join(words)\n",
        "\n",
        "noisy_text = random_word_swap(original_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPrvsnpKZrvJ"
      },
      "source": [
        "### 5.4 **Data Stratification**\n",
        "   - In order to make sure your model does not overfit to specific patterns (e.g., certain topics or styles dominating the training data), stratify your data by categories like topic, length, or language. This ensures that all categories are evenly represented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfn41dGAZsJ9"
      },
      "outputs": [],
      "source": [
        "def stratify_by_length(texts, bins=5):\n",
        "    text_lengths = [len(text.split()) for text in texts]\n",
        "    bin_thresholds = np.histogram(text_lengths, bins=bins)[1]\n",
        "    stratified_data = [[] for _ in range(bins)]\n",
        "\n",
        "    for text in texts:\n",
        "        for i in range(len(bin_thresholds)-1):\n",
        "            if bin_thresholds[i] < len(text.split()) <= bin_thresholds[i+1]:\n",
        "                stratified_data[i].append(text)\n",
        "                break\n",
        "\n",
        "    return stratified_data\n",
        "\n",
        "stratified_texts = stratify_by_length(list_of_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxombqmLZslU"
      },
      "source": [
        "### 5.5 **Data Balancing (Avoiding Bias)**\n",
        "   - Make sure your dataset is not biased toward certain topics, terms, or cultural contexts. This can be done by balancing categories (e.g., gender, race, profession) within the training corpus.\n",
        "   - Use topic modeling or clustering to detect if your dataset has an over-representation of certain topics, then down-sample or up-sample as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RgVzGTcZs78"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from random import shuffle\n",
        "\n",
        "def downsample_data_by_category(texts, categories, max_per_category=1000):\n",
        "    category_count = Counter(categories)\n",
        "    sampled_texts = []\n",
        "    for category in category_count:\n",
        "        texts_in_category = [text for text, cat in zip(texts, categories) if cat == category]\n",
        "        shuffle(texts_in_category)\n",
        "        sampled_texts.extend(texts_in_category[:max_per_category])\n",
        "    return sampled_texts\n",
        "\n",
        "sampled_texts = downsample_data_by_category(list_of_texts, list_of_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNcOqAJvaUU0"
      },
      "source": [
        "### 5.6 **Semantic Clustering and Oversampling**\n",
        "   - If certain topics or categories in your data are underrepresented, you can use semantic clustering to identify and oversample these categories to ensure the model doesn't ignore them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJyuCxncaUnx"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(list_of_texts)\n",
        "\n",
        "# Build a Faiss index for fast clustering\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# Perform clustering to find semantic clusters\n",
        "D, I = index.search(embeddings, k=5)  # k is the number of nearest neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xN0YVJvanTn"
      },
      "source": [
        "### 5.7 **Adversarial Data Training**\n",
        "   - Train your model with adversarial examples to help it become more robust. This can include slight perturbations to the input data or feeding it challenging cases (e.g., purposely ambiguous or contradicting text).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWR1SAK5aoCB"
      },
      "outputs": [],
      "source": [
        "def add_perturbation(text):\n",
        "    return ''.join([char if random.random() > 0.05 else random.choice(string.ascii_lowercase) for char in text])\n",
        "\n",
        "perturbed_text = add_perturbation(original_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCHrk3LtYq-G"
      },
      "source": [
        "## 6. Implement a Text `Tokenizer`\n",
        "\n",
        "As is well known, there is a significant gap between human language and machine understanding. Therefore, when processing language text with computers, the first step is to convert the raw text into a format that computers can effectively handle. This process is called tokenization. Tokenization involves breaking down text into smaller, more manageable units (called tokens), which can be words, subwords, or even individual characters.\n",
        "\n",
        "In the vast majority of scenarios, the tokenizers we use include Byte-Pair Encoding (BPE), used by GPT-2; WordPiece, used by BERT; and the SentencePiece tokenizer used by T5, which was developed to address the situation where languages do not use spaces to separate words.\n",
        "\n",
        "It should be noted that the Tiktoken tokenizer used by OpenAI models has recently gained significant attention.\n",
        "\n",
        "In this tutorial, we first go through the BPE methods and then we discuss about the `Tiktoken` method.\n",
        "\n",
        "### 6.1 Train a BPE Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uI3P6HFggoZ_"
      },
      "outputs": [],
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "23aa3a4ce6134c44bd623373260de914",
            "c23ceaf284ba4199a6807cec9206a46d",
            "c1c57ccbfa734469b96796537e982cb3",
            "7472b198e2c94ef5bc72dc7d458c0823",
            "1024cff6084140dabb2f05332e0fa353",
            "79a51ac7ed764bf0bd2b240d783f27b5",
            "ff1aecd247ab441383f1d7e09eff1514",
            "1524b881d4f2457a91c4b3fda9c1bf78",
            "196b3cd9236f4b968fee8bef7bba61ba",
            "44197147d3534edc9bd4736df5e50ecc",
            "2ca33379389a43038290ca8f17ddae60",
            "43745813c0b844099089a3dc9ad77398",
            "067748c687d447a6a1e27f70466d9fb0",
            "bd37dd6b4e914eff839919a33d1d69c4",
            "7274b72a9e7b43d4b5eae94762562989",
            "4cc09fce4876437aad4396e13d3b4a26",
            "bdc02b6a18504e9199bb7597a5bc3b5a",
            "9597bca68db847c4aa77e8c3c9f5b1bf",
            "f7ae93bc552d498aa85bcae1eda64988",
            "609bdd93fa054458bf680be86f1c998d",
            "7380abd7188a4c3897c9f48d61252e1e",
            "6ff15ccd9d05492c8cf16f1c624b05d6",
            "81139d555baf48e7b3b0ff0cd23bdaf5",
            "97360835bc764a73a0e541dc3e5976c5",
            "45a349a2c4ca449da080b7ca72acde60",
            "55725db313cc4c5589881849c235b40b",
            "ffd4ff06f97e4f9bbfed5e4edf71fa82",
            "2700a56daa5040b2913ad09b10d066d0",
            "780b066fcbbf4cf0a4ed87cea66a16b4",
            "3b16c88bd6594f6bb6eb1ec95e487fa7",
            "fee5468326e647dfbaf85ebbf8c029a4",
            "dad7764f5b414cb4afd976c8d6439379",
            "1f272fdf08594dd58993316d3cd7d505"
          ]
        },
        "id": "WqV5syXmSUO2",
        "outputId": "5f087706-51bc-4a2a-f8c2-f65614504c16"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"Syed-Hasan-8503/cosmopedia-10k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bF6mTN78gfg"
      },
      "outputs": [],
      "source": [
        "train_data = ds['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuUbEsLrhGgV"
      },
      "outputs": [],
      "source": [
        "def get_training_corpus(dataset):\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTkYIeWo9qAX"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybdc3W1O9qd7"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.StripAccents()] #normalizers.Lowercase()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaBFwu7y9qlF",
        "outputId": "d81fe800-9b10-49b6-9ca3-63544c457616"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAed7AZQBg4a"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK7tQxUeBhNc",
        "outputId": "035dd017-0213-4f81-cb2e-c7a1fcb568da"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXQtPTkvBhXQ"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(train_data), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKERsBfWBhl9",
        "outputId": "828df2e5-7b6f-4eef-8fdc-777e14e6d235"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY_hUUbrCJ7W"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sZNHR7ytCKes",
        "outputId": "fb7e5bda-fca3-4c7e-f239-e142a8bf251c"
      },
      "outputs": [],
      "source": [
        "sentence = \"Let's test this tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jkVY_DUBCOm9",
        "outputId": "a483a0b5-4d02-4299-ce03-85d12e238d95"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh5x7Kzv9h5a"
      },
      "source": [
        "Hooray!üéâ We have trained a tokenizer from scratch.\n",
        "\n",
        "In most cases, different large language models (LLMs) have different tokenizers. However, some research points out that even though tokenizers vary from model to model‚Äîleading to different models having different embedding tables‚Äîthey result in relatively similar semantic representations [3]. Therefore, exploring this phenomenon is a great research direction.\n",
        "\n",
        "### 6.2 Deep Dive into `Tiktoken`\n",
        "\n",
        "Both Llama3 and OpenAI use `Tiktoken` BPE, while Llama2 uses `sentencepiece` BPE. The big difference between `TikToken` BPE and `sentencepiece` BPE is that `TikToken` BPE doesn't always split words into smaller parts if the whole word is already known. For example, if **\"sentence\"** is in the vocabulary, it stays as one token instead of splitting into **[\"sent\",\"ence\"]**. One can try on it on [tiktokenizer website app](https://tiktokenizer.vercel.app/)\n",
        "\n",
        "First, let's take a look at the basic usage of the `Tiktoken` as follows (directly paste from the OpenAI tiktoken [Repo](https://github.com/openai/tiktoken)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SJeBf7-ohKY5",
        "outputId": "cbb47ecc-6265-4165-b3b0-2e04e299b425"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWYM4rlvhH9f"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
        "\n",
        "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37SXD8yY_Fle"
      },
      "source": [
        "Now, let's take a look at the tokenizer of \"gpt-4o\", quote [source code](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/core.py#L324) from `Tiktoken`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pSf6eKr_nhI"
      },
      "outputs": [],
      "source": [
        "# Get the vocabulary (token string to token ID mapping)\n",
        "gpt4o_vocab = enc.token_byte_values()\n",
        "gpt4o_vocab_dict = enc._mergeable_ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx8MSZgD-sq_",
        "outputId": "190057f9-298c-46a8-e913-ebe9e5175e86"
      },
      "outputs": [],
      "source": [
        "# Get the special token\n",
        "enc._special_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwBXrOEi_oMt"
      },
      "source": [
        "As mentioned in Llama3 paper [6], Meta claims that Llama3 uses a vocabulary with 128K tokens, combineing **100K tokens from the tiktoken\n",
        "tokenizer** with **28K additional tokens to better support non-English languages.** Some we can just use `Tiktoken` and combine our new tokens (Chinese tokens), and refer to the README of [extending tiktoken](https://github.com/openai/tiktoken/tree/main?tab=readme-ov-file#extending-tiktoken)\n",
        "\n",
        "> Notice: Turn to [https://huggingface.co/docs/transformers/main/en/tiktoken](https://huggingface.co/docs/transformers/main/en/tiktoken) to see more example using package `transformers` and `tiktoken`.\n",
        "\n",
        "\n",
        "## What's More\n",
        "\n",
        "Besides what we discuss in this notebook, we also recomman the following aspects in data preprocessing.\n",
        "\n",
        "### Monitoring Data Use in Pretraining\n",
        "\n",
        "1. **Sampling Strategies**:\n",
        "   - Use stratified or weighted sampling to ensure the model learns from all data sources proportionally.\n",
        "   - For domain-specific models, prioritize domain data while also incorporating some general data for broader knowledge.\n",
        "\n",
        "2. **Data Usage Policies**:\n",
        "   - Ensure compliance with licenses (e.g., Creative Commons, Open Data, or public domain data).\n",
        "   - Set clear guidelines on the use of sensitive data (e.g., data involving personal information, confidential data).\n",
        "\n",
        "### Final Considerations:\n",
        "\n",
        "- **Balanced Approach**: While diversity in data is crucial for general-purpose models, domain-specific models benefit from a fine balance between general data and specialized data.\n",
        "- **Efficient Preprocessing**: Spend time ensuring that the data is clean, diverse, and high-quality before pretraining. It‚Äôs more efficient to preprocess properly upfront than to deal with performance issues during or after training.\n",
        "- **Ethical Awareness**: Ensure that the pretraining data aligns with ethical guidelines, and be aware of potential biases introduced by the data.\n",
        "\n",
        "By carefully selecting and preparing your dataset, you will set a strong foundation for training your LLM, leading to better performance, generalization, and fairness in downstream applications.\n",
        "\n",
        "\n",
        "## Reference\n",
        "\n",
        "1. https://github.com/jiangnanboy/llm_corpus_quality\n",
        "2. https://huggingface.co/blog/cosmopedia\n",
        "3. [The Platonic Representation Hypothesis](https://arxiv.org/pdf/2405.07987)\n",
        "4. https://huggingface.co/learn/nlp-course/en/chapter6/2\n",
        "5. https://huggingface.co/learn/nlp-course/en/chapter6/5"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "067748c687d447a6a1e27f70466d9fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdc02b6a18504e9199bb7597a5bc3b5a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9597bca68db847c4aa77e8c3c9f5b1bf",
            "value": "train-00000-of-00001.parquet:‚Äá100%"
          }
        },
        "1024cff6084140dabb2f05332e0fa353": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1524b881d4f2457a91c4b3fda9c1bf78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196b3cd9236f4b968fee8bef7bba61ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f272fdf08594dd58993316d3cd7d505": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23aa3a4ce6134c44bd623373260de914": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c23ceaf284ba4199a6807cec9206a46d",
              "IPY_MODEL_c1c57ccbfa734469b96796537e982cb3",
              "IPY_MODEL_7472b198e2c94ef5bc72dc7d458c0823"
            ],
            "layout": "IPY_MODEL_1024cff6084140dabb2f05332e0fa353"
          }
        },
        "2700a56daa5040b2913ad09b10d066d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca33379389a43038290ca8f17ddae60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b16c88bd6594f6bb6eb1ec95e487fa7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43745813c0b844099089a3dc9ad77398": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_067748c687d447a6a1e27f70466d9fb0",
              "IPY_MODEL_bd37dd6b4e914eff839919a33d1d69c4",
              "IPY_MODEL_7274b72a9e7b43d4b5eae94762562989"
            ],
            "layout": "IPY_MODEL_4cc09fce4876437aad4396e13d3b4a26"
          }
        },
        "44197147d3534edc9bd4736df5e50ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a349a2c4ca449da080b7ca72acde60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b16c88bd6594f6bb6eb1ec95e487fa7",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fee5468326e647dfbaf85ebbf8c029a4",
            "value": 10000
          }
        },
        "4cc09fce4876437aad4396e13d3b4a26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55725db313cc4c5589881849c235b40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dad7764f5b414cb4afd976c8d6439379",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1f272fdf08594dd58993316d3cd7d505",
            "value": "‚Äá10000/10000‚Äá[00:00&lt;00:00,‚Äá30938.23‚Äáexamples/s]"
          }
        },
        "609bdd93fa054458bf680be86f1c998d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ff15ccd9d05492c8cf16f1c624b05d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7274b72a9e7b43d4b5eae94762562989": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7380abd7188a4c3897c9f48d61252e1e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6ff15ccd9d05492c8cf16f1c624b05d6",
            "value": "‚Äá30.6M/30.6M‚Äá[00:01&lt;00:00,‚Äá27.6MB/s]"
          }
        },
        "7380abd7188a4c3897c9f48d61252e1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7472b198e2c94ef5bc72dc7d458c0823": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44197147d3534edc9bd4736df5e50ecc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2ca33379389a43038290ca8f17ddae60",
            "value": "‚Äá468/468‚Äá[00:00&lt;00:00,‚Äá19.2kB/s]"
          }
        },
        "780b066fcbbf4cf0a4ed87cea66a16b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79a51ac7ed764bf0bd2b240d783f27b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81139d555baf48e7b3b0ff0cd23bdaf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97360835bc764a73a0e541dc3e5976c5",
              "IPY_MODEL_45a349a2c4ca449da080b7ca72acde60",
              "IPY_MODEL_55725db313cc4c5589881849c235b40b"
            ],
            "layout": "IPY_MODEL_ffd4ff06f97e4f9bbfed5e4edf71fa82"
          }
        },
        "9597bca68db847c4aa77e8c3c9f5b1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97360835bc764a73a0e541dc3e5976c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2700a56daa5040b2913ad09b10d066d0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_780b066fcbbf4cf0a4ed87cea66a16b4",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
          }
        },
        "bd37dd6b4e914eff839919a33d1d69c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ae93bc552d498aa85bcae1eda64988",
            "max": 30634550,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_609bdd93fa054458bf680be86f1c998d",
            "value": 30634550
          }
        },
        "bdc02b6a18504e9199bb7597a5bc3b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c57ccbfa734469b96796537e982cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1524b881d4f2457a91c4b3fda9c1bf78",
            "max": 468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_196b3cd9236f4b968fee8bef7bba61ba",
            "value": 468
          }
        },
        "c23ceaf284ba4199a6807cec9206a46d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79a51ac7ed764bf0bd2b240d783f27b5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ff1aecd247ab441383f1d7e09eff1514",
            "value": "README.md:‚Äá100%"
          }
        },
        "dad7764f5b414cb4afd976c8d6439379": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ae93bc552d498aa85bcae1eda64988": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee5468326e647dfbaf85ebbf8c029a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff1aecd247ab441383f1d7e09eff1514": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffd4ff06f97e4f9bbfed5e4edf71fa82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
